{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score,\n",
    "    precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data_train = pd.read_csv('../files/input/train_data.csv.zip', index_col= False, compression=\"zip\")\n",
    "data_test = pd.read_csv('../files/input/test_data.csv.zip', index_col= False, compression=\"zip\")\n",
    "columnas_diferentes = ['PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6']\n",
    "\n",
    "print(data_train.isnull().sum())\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data_train.rename(columns={\"default payment next month\" : \"default\"}, inplace=True)\n",
    "data_test.rename(columns={\"default payment next month\" : \"default\"}, inplace=True)\n",
    "data_train.drop(columns=[\"ID\"], inplace=True)\n",
    "data_test.drop(columns=[\"ID\"], inplace=True)\n",
    "\n",
    "data_train['EDUCATION'] = data_train['EDUCATION'].apply(lambda x: 4 if x > 4 else x)\n",
    "data_test['EDUCATION'] = data_test['EDUCATION'].apply(lambda x: 4 if x > 4 else x)\n",
    "\n",
    "data_train['EDUCATION'] = data_train['EDUCATION'].apply(lambda x: x if x > 0 else np.nan)\n",
    "data_test['EDUCATION'] = data_test['EDUCATION'].apply(lambda x: x if x > 0 else np.nan)\n",
    "\n",
    "data_train['MARRIAGE'] = data_train['MARRIAGE'].apply(lambda x: x if x > 0 else np.nan)\n",
    "data_test['MARRIAGE'] = data_test['MARRIAGE'].apply(lambda x: x if x > 0 else np.nan)\n",
    "\n",
    "pay_columns = ['PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6']\n",
    "\n",
    "data_train[pay_columns] = data_train[pay_columns].applymap(lambda x: x if x >= 0 else np.nan)\n",
    "data_test[pay_columns] = data_test[pay_columns].applymap(lambda x: x if x >= 0 else np.nan)\n",
    "\n",
    "data_train.dropna(inplace=True)\n",
    "data_test.dropna(inplace=True)\n",
    "\n",
    "data_train = data_train.astype(int)\n",
    "data_test = data_test.astype(int)\n",
    "\n",
    "print(data_test.isnull().sum())\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Dividir en características (X) y etiquetas (y)\n",
    "X_train = data_train.drop(columns=\"default\")\n",
    "y_train = data_train[\"default\"]\n",
    "\n",
    "X_test = data_test.drop(columns=\"default\")\n",
    "y_test = data_test[\"default\"]\n",
    "\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Identificar las variables categóricas y numéricas\n",
    "categorical_features = ['SEX', 'EDUCATION', 'MARRIAGE']\n",
    "numerical_features = ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "\n",
    "\n",
    "\n",
    "# Preprocesamiento para las variables categóricas\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "# Preprocesamiento para las variables numéricas\n",
    "numerical_transformer = MinMaxScaler()\n",
    "\n",
    "# Combinación de preprocesadores\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers= [\n",
    "        (\"encoder\", categorical_transformer, categorical_features),\n",
    "        (\"numerica\", numerical_transformer, numerical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Creación del Pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"feature_selection\", SelectKBest(score_func=f_classif, k=10)),\n",
    "    (\"classifier\", LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "# Definición del grid de hiperparámetros\n",
    "param_grid = {\n",
    "    \"feature_selection__k\": [5, 7, 10],\n",
    "    \"classifier__C\": [0.1, 1, 10],\n",
    "    \"classifier__solver\": [\"liblinear\", \"lbfgs\"]\n",
    "}\n",
    "\n",
    "# Validación cruzada con 10 splits\n",
    "model = GridSearchCV(pipeline, param_grid, cv=10, scoring=\"balanced_accuracy\", n_jobs=-1, refit=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score,\n",
    "    precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Mejor modelo\n",
    "print(\"Mejores parámetros: \", model.best_params_)\n",
    "best_model = model.best_estimator_\n",
    "\n",
    "\n",
    "#Guardar el modelo\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# Ruta del directorio donde se guardará el archivo\n",
    "dir_path = '../files/models'\n",
    "\n",
    "# Verificar si el directorio existe, si no, crearlo\n",
    "if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "# Ruta del archivo GZIP\n",
    "gzip_file_path = os.path.join(dir_path, 'model.pkl.gz')\n",
    "\n",
    "# Guardar el modelo comprimido como un archivo GZIP\n",
    "with gzip.open(gzip_file_path, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(f\"Modelo guardado correctamente en {gzip_file_path}\")\n",
    "\n",
    "\n",
    "# Predicciones\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Métricas\n",
    "\n",
    "metrics = {\n",
    "    \"Train\": {\n",
    "        \"Accuracy\": accuracy_score(y_train, y_train_pred),\n",
    "        \"Balanced accuracy\": balanced_accuracy_score(y_train, y_train_pred),\n",
    "        \"Precision\": precision_score(y_train, y_train_pred),\n",
    "        \"Recall\": recall_score(y_train, y_train_pred),\n",
    "        \"F1-Score\": f1_score(y_train, y_train_pred)\n",
    "    },\n",
    "    \"Test\":{\n",
    "        \"Accuracy\": accuracy_score(y_test, y_test_pred),\n",
    "        \"Balanced accuracy\": balanced_accuracy_score(y_test, y_test_pred),\n",
    "        \"Precision\": precision_score(y_test, y_test_pred),\n",
    "        \"Recall\": recall_score(y_test, y_test_pred),\n",
    "        \"F1-Score\": f1_score(y_test, y_test_pred)\n",
    "    }\n",
    "}\n",
    "\n",
    "print(metrics)\n",
    "\n",
    "\n",
    "# Matriz de Confusión\n",
    "for dataset, y_true, y_pred in [(\"Train\", y_train, y_train_pred), (\" Test\", y_test, y_test_pred)]:\n",
    "  cm = confusion_matrix(y_true, y_pred)\n",
    "  print(f\"Matriz de confusión ({dataset}):\\n\", cm)\n",
    "\n",
    "\n",
    "\n",
    "#crear json\n",
    "\n",
    "import json\n",
    "\n",
    "# Lista para almacenar las líneas del archivo JSON\n",
    "results = []\n",
    "\n",
    "# Agregar información de metrics para train y test\n",
    "for dataset in metrics:\n",
    "    results.append({\n",
    "        'type': 'metrics',\n",
    "        'dataset': dataset.lower(),\n",
    "        'precision': float(metrics[dataset]['Precision']), \n",
    "        'balanced_accuracy': float(metrics[dataset]['Balanced accuracy']),       \n",
    "        'recall': float(metrics[dataset]['Recall']),\n",
    "        'f1_score': float(metrics[dataset]['F1-Score'])\n",
    "    })\n",
    "\n",
    "# Generar las matrices de confusión para train y test\n",
    "for dataset, y_true, y_pred in [(\"Train\", y_train, y_train_pred), (\"Test\", y_test, y_test_pred)]:\n",
    "    # Calculamos la matriz de confusión\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Organizar la matriz de confusión en un diccionario\n",
    "    cm_dict = {\n",
    "        \"type\": \"cm_matrix\",\n",
    "        \"dataset\": dataset.lower(),  # 'train' o 'test'\n",
    "        \"true_0\": {\"predicted_0\": cm[0, 0], \"predicted_1\": cm[0, 1]},\n",
    "        \"true_1\": {\"predicted_0\": cm[1, 0], \"predicted_1\": cm[1, 1]}\n",
    "    }\n",
    "    \n",
    "    # Agregar la matriz de confusión a la lista de resultados\n",
    "    results.append(cm_dict)\n",
    "\n",
    "\n",
    "print(results)\n",
    "\n",
    "\n",
    "\n",
    "#guardar json\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Ruta donde se guardará el archivo JSON\n",
    "output_path = \"../files/output\"\n",
    "\n",
    "# Crear la carpeta de salida si no existe\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Función para convertir tipos de datos de numpy (int64, float64) a tipos estándar de Python\n",
    "def convert_numpy_types(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy_types(item) for item in obj]\n",
    "    elif isinstance(obj, np.int64):  # Si el valor es un int64 de numpy\n",
    "        return int(obj)  # Convertir a tipo int de Python\n",
    "    elif isinstance(obj, np.float64):  # Si el valor es un float64 de numpy\n",
    "        return float(obj)  # Convertir a tipo float de Python\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Guardar cada elemento en una línea separada del archivo JSON\n",
    "with open('../files/output/metrics.json', 'w', encoding='utf-8') as f:  # Abrir en modo texto con codificación UTF-8\n",
    "    for result in results:\n",
    "        result = convert_numpy_types(result)  # Convertir los valores de int64 y float64 a tipos estándar\n",
    "        json.dump(result, f, ensure_ascii=False)  # Escribir el objeto en formato JSON\n",
    "        f.write('\\n')  # Escribir un salto de línea después de cada línea\n",
    "\n",
    "print(f\"Archivo guardado correctamente en {output_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score,\n",
    "    precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta al archivo ZIP\n",
    "zip_path = \"../files/input/train_data.csv.zip\"\n",
    "\n",
    "# Abrir el archivo ZIP para listar su contenido y leer el archivo CSV\n",
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    # Obtener la lista de archivos dentro del ZIP\n",
    "    file_list = z.namelist()\n",
    "    \n",
    "    # Buscar el primer archivo con extensión .csv\n",
    "    csv_filename = next((file for file in file_list if file.endswith('.csv')), None)\n",
    "\n",
    "    # Validar si se encontró un archivo CSV\n",
    "    if csv_filename:       \n",
    "        # Leer el archivo CSV dentro del ZIP\n",
    "        with z.open(csv_filename) as f:\n",
    "            dftrain = pd.read_csv(f)      \n",
    "\n",
    "    else:\n",
    "        print(\"No se encontró un archivo CSV dentro del archivo ZIP.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.rename(columns={'default payment next month': 'default'}, inplace=True)\n",
    "dftrain = dftrain.drop(columns=[\"ID\"])\n",
    "dftrain = dftrain.dropna()\n",
    "# Reemplazar valores mayores a 4 con \"others\"\n",
    "dftrain['EDUCATION'] = dftrain['EDUCATION'].apply(lambda x: 5 if x > 4 else x)\n",
    "dftrain['EDUCATION'] = dftrain['EDUCATION'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta al archivo ZIP\n",
    "zip_path = \"../files/input/test_data.csv.zip\"\n",
    "\n",
    "# Abrir el archivo ZIP para listar su contenido y leer el archivo CSV\n",
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    # Obtener la lista de archivos dentro del ZIP\n",
    "    file_list = z.namelist()\n",
    "    \n",
    "    # Buscar el primer archivo con extensión .csv\n",
    "    csv_filename = next((file for file in file_list if file.endswith('.csv')), None)\n",
    "\n",
    "    # Validar si se encontró un archivo CSV\n",
    "    if csv_filename:       \n",
    "        # Leer el archivo CSV dentro del ZIP\n",
    "        with z.open(csv_filename) as f:\n",
    "            dftest = pd.read_csv(f)      \n",
    "\n",
    "    else:\n",
    "        print(\"No se encontró un archivo CSV dentro del archivo ZIP.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest.rename(columns={'default payment next month': 'default'}, inplace=True)\n",
    "dftest = dftest.drop(columns=[\"ID\"])\n",
    "dftest = dftest.dropna()\n",
    "# Reemplazar valores mayores a 4 con \"others\"\n",
    "dftest['EDUCATION'] = dftest['EDUCATION'].apply(lambda x: 4 if x > 4 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en características (X) y etiquetas (y)\n",
    "X_train = dftrain.drop(columns=\"default\")\n",
    "y_train = dftrain[\"default\"]\n",
    "\n",
    "X_test = dftest.drop(columns=\"default\")\n",
    "y_test = dftest[\"default\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Transforma las variables categoricas usando el método\n",
    "#   one-hot-encoding.\n",
    "\n",
    "# Identificar las variables categóricas y numéricas\n",
    "categorical_features = ['SEX', 'EDUCATION', 'MARRIAGE']\n",
    "numerical_features = ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "\n",
    "\n",
    "\n",
    "# Preprocesamiento para las variables categóricas\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "# Preprocesamiento para las variables numéricas\n",
    "numerical_transformer = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinación de preprocesadores\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers= [\n",
    "        (\"encoder\", categorical_transformer, categorical_features),\n",
    "        (\"numerica\", numerical_transformer, numerical_features)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del Pipeline\n",
    "#pipeline = Pipeline([\n",
    "#    (\"preprocessor\", preprocessor),\n",
    "#    (\"feature_selection\", SelectKBest(score_func=f_classif, k=10)),\n",
    "#    (\"classifier\", LogisticRegression(max_iter=1000, random_state=42))\n",
    "#])\n",
    "\n",
    "# Creación del Pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),  # Tu preprocesador (por ejemplo, ColumnTransformer)\n",
    "    (\"feature_selection\", SelectKBest(score_func=f_classif)),  # Sin especificar k aún\n",
    "    (\"classifier\", LogisticRegression(max_iter=1000, random_state=42))  # Clasificador\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['EDUCATION'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del grid de hiperparámetros\n",
    "\n",
    "# Optimice los hiperparametros del pipeline usando validación cruzada.\n",
    "# Use 10 splits para la validación cruzada. Use la función de precision\n",
    "# balanceada para medir la precisión del modelo.\n",
    "#\n",
    "\n",
    "param_grid = {\n",
    "    \"feature_selection__k\": [5, 10, 15, 20],\n",
    "    \"classifier__C\": [0.01, 0.1, 1, 10],\n",
    "    \"classifier__solver\": [\"liblinear\", \"saga\"] #\"lbfgs\"]\n",
    "}\n",
    "\n",
    "# Validación cruzada con 10 splits\n",
    "\n",
    "# Aplicar GridSearchCV para encontrar el mejor modelo\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=10,  scoring=\"balanced_accuracy\", verbose=1, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GUARDAR EL MODELO\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# Ruta del directorio donde se guardará el archivo\n",
    "dir_path = '../files/models'\n",
    "\n",
    "# Verificar si el directorio existe, si no, crearlo\n",
    "if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "# Ruta del archivo GZIP\n",
    "gzip_file_path = os.path.join(dir_path, 'model.pkl.gz')\n",
    "\n",
    "# Guardar el modelo comprimido como un archivo GZIP\n",
    "with gzip.open(gzip_file_path, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "print(f\"Modelo guardado correctamente en {gzip_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Métricas\n",
    "\n",
    "metrics = {\n",
    "    \"Train\": {\n",
    "        \"Accuracy\": accuracy_score(y_train, y_train_pred),\n",
    "        \"Balanced accuracy\": balanced_accuracy_score(y_train, y_train_pred),\n",
    "        \"Precision\": precision_score(y_train, y_train_pred),\n",
    "        \"Recall\": recall_score(y_train, y_train_pred),\n",
    "        \"F1-Score\": f1_score(y_train, y_train_pred)\n",
    "    },\n",
    "    \"Test\":{\n",
    "        \"Accuracy\": accuracy_score(y_test, y_test_pred),\n",
    "        \"Balanced accuracy\": balanced_accuracy_score(y_test, y_test_pred),\n",
    "        \"Precision\": precision_score(y_test, y_test_pred),\n",
    "        \"Recall\": recall_score(y_test, y_test_pred),\n",
    "        \"F1-Score\": f1_score(y_test, y_test_pred)\n",
    "    }\n",
    "}\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de Confusión\n",
    "for dataset, y_true, y_pred in [(\"Train\", y_train, y_train_pred), (\" Test\", y_test, y_test_pred)]:\n",
    "  cm = confusion_matrix(y_true, y_pred)\n",
    "  print(f\"Matriz de confusión ({dataset}):\\n\", cm)\n",
    "  #disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\n",
    "  #disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Lista para almacenar las líneas del archivo JSON\n",
    "results = []\n",
    "\n",
    "# Agregar información de metrics para train y test\n",
    "for dataset in metrics:\n",
    "    results.append({\n",
    "        'type': 'metrics',\n",
    "        'dataset': dataset.lower(),\n",
    "        'precision': float(metrics[dataset]['Precision']), \n",
    "        'balanced_accuracy': float(metrics[dataset]['Balanced accuracy']),       \n",
    "        'recall': float(metrics[dataset]['Recall']),\n",
    "        'f1_score': float(metrics[dataset]['F1-Score'])\n",
    "    })\n",
    "\n",
    "# Generar las matrices de confusión para train y test\n",
    "for dataset, y_true, y_pred in [(\"Train\", y_train, y_train_pred), (\"Test\", y_test, y_test_pred)]:\n",
    "    # Calculamos la matriz de confusión\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Organizar la matriz de confusión en un diccionario\n",
    "    cm_dict = {\n",
    "        \"type\": \"cm_matrix\",\n",
    "        \"dataset\": dataset.lower(),  # 'train' o 'test'\n",
    "        \"true_0\": {\"predicted_0\": cm[0, 0], \"predicted_1\": cm[0, 1]},\n",
    "        \"true_1\": {\"predicted_0\": cm[1, 0], \"predicted_1\": cm[1, 1]}\n",
    "    }\n",
    "    \n",
    "    # Agregar la matriz de confusión a la lista de resultados\n",
    "    results.append(cm_dict)\n",
    "\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Ruta donde se guardará el archivo JSON\n",
    "output_path = \"../files/output\"\n",
    "\n",
    "# Crear la carpeta de salida si no existe\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Función para convertir tipos de datos de numpy (int64, float64) a tipos estándar de Python\n",
    "def convert_numpy_types(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy_types(item) for item in obj]\n",
    "    elif isinstance(obj, np.int64):  # Si el valor es un int64 de numpy\n",
    "        return int(obj)  # Convertir a tipo int de Python\n",
    "    elif isinstance(obj, np.float64):  # Si el valor es un float64 de numpy\n",
    "        return float(obj)  # Convertir a tipo float de Python\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Guardar cada elemento en una línea separada del archivo JSON\n",
    "with open('../files/output/metrics.json', 'w', encoding='utf-8') as f:  # Abrir en modo texto con codificación UTF-8\n",
    "    for result in results:\n",
    "        result = convert_numpy_types(result)  # Convertir los valores de int64 y float64 a tipos estándar\n",
    "        json.dump(result, f, ensure_ascii=False)  # Escribir el objeto en formato JSON\n",
    "        f.write('\\n')  # Escribir un salto de línea después de cada línea\n",
    "\n",
    "print(f\"Archivo guardado correctamente en {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score,\n",
    "    precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data_train = pd.read_csv('../files/input/train_data.csv.zip', index_col= False, compression=\"zip\")\n",
    "data_test = pd.read_csv('../files/input/test_data.csv.zip', index_col= False, compression=\"zip\")\n",
    "columnas_diferentes = ['PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6']\n",
    "\n",
    "print(data_train.isnull().sum())\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data_train.rename(columns={\"default payment next month\" : \"default\"}, inplace=True)\n",
    "data_test.rename(columns={\"default payment next month\" : \"default\"}, inplace=True)\n",
    "data_train.drop(columns=[\"ID\"], inplace=True)\n",
    "data_test.drop(columns=[\"ID\"], inplace=True)\n",
    "\n",
    "data_train['EDUCATION'] = data_train['EDUCATION'].apply(lambda x: 4 if x > 4 else x)\n",
    "data_test['EDUCATION'] = data_test['EDUCATION'].apply(lambda x: 4 if x > 4 else x)\n",
    "\n",
    "data_train['EDUCATION'] = data_train['EDUCATION'].apply(lambda x: x if x > 0 else np.nan)\n",
    "data_test['EDUCATION'] = data_test['EDUCATION'].apply(lambda x: x if x > 0 else np.nan)\n",
    "\n",
    "data_train['MARRIAGE'] = data_train['MARRIAGE'].apply(lambda x: x if x > 0 else np.nan)\n",
    "data_test['MARRIAGE'] = data_test['MARRIAGE'].apply(lambda x: x if x > 0 else np.nan)\n",
    "\n",
    "pay_columns = ['PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6']\n",
    "\n",
    "data_train[pay_columns] = data_train[pay_columns].applymap(lambda x: x if x >= 0 else np.nan)\n",
    "data_test[pay_columns] = data_test[pay_columns].applymap(lambda x: x if x >= 0 else np.nan)\n",
    "\n",
    "data_train.dropna(inplace=True)\n",
    "data_test.dropna(inplace=True)\n",
    "\n",
    "data_train = data_train.astype(int)\n",
    "data_test = data_test.astype(int)\n",
    "\n",
    "print(data_test.isnull().sum())\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Dividir en características (X) y etiquetas (y)\n",
    "X_train = data_train.drop(columns=\"default\")\n",
    "y_train = data_train[\"default\"]\n",
    "\n",
    "X_test = data_test.drop(columns=\"default\")\n",
    "y_test = data_test[\"default\"]\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Identificar las variables categóricas y numéricas\n",
    "categorical_features = ['SEX', 'EDUCATION', 'MARRIAGE']\n",
    "numerical_features = ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "\n",
    "\n",
    "\n",
    "# Preprocesamiento para las variables categóricas\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "# Preprocesamiento para las variables numéricas\n",
    "numerical_transformer = MinMaxScaler()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combinación de preprocesadores\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers= [\n",
    "        (\"encoder\", categorical_transformer, categorical_features),\n",
    "        (\"numerica\", numerical_transformer, numerical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Creación del Pipeline\n",
    "#pipeline = Pipeline([\n",
    "#    (\"preprocessor\", preprocessor),\n",
    "#    (\"feature_selection\", SelectKBest(score_func=f_classif, k=10)),\n",
    "#    (\"classifier\", LogisticRegression(max_iter=1000, random_state=42, class_weight=\"balanced\"))\n",
    "#])\n",
    "\n",
    "# Creación del Pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),  # Tu preprocesador (por ejemplo, ColumnTransformer)\n",
    "    (\"feature_selection\", SelectKBest(score_func=f_classif)),  # Sin especificar k aún\n",
    "    (\"classifier\", LogisticRegression(max_iter=1000, random_state=42, C=0.01, solver='liblinear'))##LogisticRegression(max_iter=1000, random_state=42))  # Clasificador\n",
    "])\n",
    "\n",
    "\n",
    "# Definición del grid de hiperparámetros\n",
    "#param_grid = {\n",
    "#    \"feature_selection__k\": [5, 7, 10],\n",
    "#    \"classifier__C\": [0.1, 1, 10],\n",
    "#    \"classifier__solver\": [\"liblinear\", \"lbfgs\"]\n",
    "#}\n",
    "\n",
    "param_grid = {\n",
    "    \"feature_selection__k\": [5, 7, 10, 15, 20],\n",
    "    \"classifier__C\": [0.01, 0.1, 1, 10, 100],\n",
    "    \"classifier__solver\": [\"liblinear\", \"lbfgs\", \"saga\"]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Validación cruzada con 10 splits\n",
    "model = GridSearchCV(pipeline, param_grid, cv=10, scoring=\"balanced_accuracy\", n_jobs=-1, refit=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score,\n",
    "    precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Mejor modelo\n",
    "print(\"Mejores parámetros: \", model.best_params_)\n",
    "best_model = model.best_estimator_\n",
    "\n",
    "# Evaluar el score en el conjunto de entrenamiento\n",
    "train_score = model.score(X_train, y_train)\n",
    "print(f\"Score en el conjunto de entrenamiento: {train_score}\")\n",
    "\n",
    "# Evaluar el score en el conjunto de test\n",
    "test_score = model.score(X_test, y_test)\n",
    "print(f\"Score en el conjunto de prueba: {test_score}\")\n",
    "\n",
    "\n",
    "# Suponiendo que 'model' es un GridSearchCV\n",
    "best_score = model.best_score_\n",
    "print(f\"Mejor puntuación obtenida: {best_score}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Hacer predicciones en el conjunto de test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Generar el reporte de clasificación\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "\n",
    "# Evaluar el score con la mejor combinación de parámetros\n",
    "best_model = model.best_estimator_\n",
    "best_train_score = best_model.score(X_train, y_train)\n",
    "best_test_score = best_model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Mejor score en entrenamiento: {best_train_score}\")\n",
    "print(f\"Mejor score en test: {best_test_score}\")\n",
    "\n",
    "\n",
    "#Guardar el modelo\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# Ruta del directorio donde se guardará el archivo\n",
    "dir_path = '../files/models'\n",
    "\n",
    "# Verificar si el directorio existe, si no, crearlo\n",
    "if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "# Ruta del archivo GZIP\n",
    "gzip_file_path = os.path.join(dir_path, 'model.pkl.gz')\n",
    "\n",
    "# Guardar el modelo comprimido como un archivo GZIP\n",
    "with gzip.open(gzip_file_path, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(f\"Modelo guardado correctamente en {gzip_file_path}\")\n",
    "\n",
    "\n",
    "# Predicciones\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Métricas\n",
    "\n",
    "metrics = {\n",
    "    \"Train\": {\n",
    "        \"Accuracy\": accuracy_score(y_train, y_train_pred),\n",
    "        \"Balanced accuracy\": balanced_accuracy_score(y_train, y_train_pred),\n",
    "        \"Precision\": precision_score(y_train, y_train_pred),\n",
    "        \"Recall\": recall_score(y_train, y_train_pred),\n",
    "        \"F1-Score\": f1_score(y_train, y_train_pred)\n",
    "    },\n",
    "    \"Test\":{\n",
    "        \"Accuracy\": accuracy_score(y_test, y_test_pred),\n",
    "        \"Balanced accuracy\": balanced_accuracy_score(y_test, y_test_pred),\n",
    "        \"Precision\": precision_score(y_test, y_test_pred),\n",
    "        \"Recall\": recall_score(y_test, y_test_pred),\n",
    "        \"F1-Score\": f1_score(y_test, y_test_pred)\n",
    "    }\n",
    "}\n",
    "\n",
    "print(metrics)\n",
    "\n",
    "# Matriz de Confusión\n",
    "for dataset, y_true, y_pred in [(\"Train\", y_train, y_train_pred), (\" Test\", y_test, y_test_pred)]:\n",
    "  cm = confusion_matrix(y_true, y_pred)\n",
    "  print(f\"Matriz de confusión ({dataset}):\\n\", cm)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Reporte de clasificación\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matriz de confusión\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "#crear json\n",
    "\n",
    "import json\n",
    "\n",
    "# Lista para almacenar las líneas del archivo JSON\n",
    "results = []\n",
    "\n",
    "# Agregar información de metrics para train y test\n",
    "for dataset in metrics:\n",
    "    results.append({\n",
    "        'type': 'metrics',\n",
    "        'dataset': dataset.lower(),\n",
    "        'precision': float(metrics[dataset]['Precision']), \n",
    "        'balanced_accuracy': float(metrics[dataset]['Balanced accuracy']),       \n",
    "        'recall': float(metrics[dataset]['Recall']),\n",
    "        'f1_score': float(metrics[dataset]['F1-Score'])\n",
    "    })\n",
    "\n",
    "# Generar las matrices de confusión para train y test\n",
    "for dataset, y_true, y_pred in [(\"Train\", y_train, y_train_pred), (\"Test\", y_test, y_test_pred)]:\n",
    "    # Calculamos la matriz de confusión\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Organizar la matriz de confusión en un diccionario\n",
    "    cm_dict = {\n",
    "        \"type\": \"cm_matrix\",\n",
    "        \"dataset\": dataset.lower(),  # 'train' o 'test'\n",
    "        \"true_0\": {\"predicted_0\": cm[0, 0], \"predicted_1\": cm[0, 1]},\n",
    "        \"true_1\": {\"predicted_0\": cm[1, 0], \"predicted_1\": cm[1, 1]}\n",
    "    }\n",
    "    \n",
    "    # Agregar la matriz de confusión a la lista de resultados\n",
    "    results.append(cm_dict)\n",
    "\n",
    "\n",
    "print(results)\n",
    "\n",
    "\n",
    "#guardar json\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Ruta donde se guardará el archivo JSON\n",
    "output_path = \"../files/output\"\n",
    "\n",
    "# Crear la carpeta de salida si no existe\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Función para convertir tipos de datos de numpy (int64, float64) a tipos estándar de Python\n",
    "def convert_numpy_types(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy_types(item) for item in obj]\n",
    "    elif isinstance(obj, np.int64):  # Si el valor es un int64 de numpy\n",
    "        return int(obj)  # Convertir a tipo int de Python\n",
    "    elif isinstance(obj, np.float64):  # Si el valor es un float64 de numpy\n",
    "        return float(obj)  # Convertir a tipo float de Python\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Guardar cada elemento en una línea separada del archivo JSON\n",
    "with open('../files/output/metrics.json', 'w', encoding='utf-8') as f:  # Abrir en modo texto con codificación UTF-8\n",
    "    for result in results:\n",
    "        result = convert_numpy_types(result)  # Convertir los valores de int64 y float64 a tipos estándar\n",
    "        json.dump(result, f, ensure_ascii=False)  # Escribir el objeto en formato JSON\n",
    "        f.write('\\n')  # Escribir un salto de línea después de cada línea\n",
    "\n",
    "print(f\"Archivo guardado correctamente en {output_path}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
